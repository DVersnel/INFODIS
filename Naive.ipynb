{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2e3e434-e665-4037-be2f-248e6ba98fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-29 11:45:05,292 - INFO - Starting to read the log file\n",
      "2024-06-29 11:45:05,482 - INFO - Completed reading and parsing the log file\n",
      "2024-06-29 11:45:05,535 - INFO - Grouped by process_id and collected log entries\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 20931/20931 [04:05<00:00, 85.19it/s]\n",
      "2024-06-29 11:49:11,530 - INFO - Filtered candidates for part 2 based on edit distance < 4\n",
      "2024-06-29 11:49:32,768 - INFO - Filtered candidates for part 1 based on edit distance < 2\n",
      "2024-06-29 11:49:42,041 - INFO - Number of clusters for part 1: 80\n",
      "2024-06-29 11:49:42,042 - INFO - Number of clusters for part 2: 7\n",
      "2024-06-29 11:49:42,042 - INFO - Average cluster size for part 1: 261.4875\n",
      "2024-06-29 11:49:42,043 - INFO - Average cluster size for part 2: 2990.0\n",
      "2024-06-29 11:49:42,043 - INFO - Merged similar groups into clusters\n",
      "2024-06-29 11:49:54,393 - INFO - Generated part1Output.txt: C:/Users/dcave/Documents/DIS_project/part1Output.txt\n",
      "2024-06-29 11:49:54,628 - INFO - Generated part1Observations.txt: C:/Users/dcave/Documents/DIS_project/part1Observations.txt\n",
      "2024-06-29 11:49:54,849 - INFO - Generated part2Observations.txt: C:/Users/dcave/Documents/DIS_project/part2Observations.txt\n",
      "2024-06-29 11:49:54,850 - INFO - Process completed.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import editdistance\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define paths\n",
    "log_file_path = \"C:/Users/dcave/Documents/DIS_project/DIS_project/data/server_log.txt\"\n",
    "part1_output_path = 'C:/Users/dcave/Documents/DIS_project/part1Output.txt'\n",
    "part1_observations_path = 'C:/Users/dcave/Documents/DIS_project/part1Observations.txt'\n",
    "part2_observations_path = 'C:/Users/dcave/Documents/DIS_project/part2Observations.txt'\n",
    "\n",
    "logging.info(\"Starting to read the log file\")\n",
    "\n",
    "# Read log file into a list of dictionaries (first 1000 lines)\n",
    "log_entries = []\n",
    "with open(log_file_path, 'r') as file:\n",
    "    for i, line in enumerate(file):\n",
    "        if i >= 200000:\n",
    "            break\n",
    "        from_server, to_server, time, action, process_id = line.strip().split(',')\n",
    "        log_entries.append({\n",
    "            \"from_server\": from_server,\n",
    "            \"to_server\": to_server,\n",
    "            \"time\": int(time),\n",
    "            \"action\": action.strip(),\n",
    "            \"process_id\": int(process_id)\n",
    "        })\n",
    "\n",
    "logging.info(\"Completed reading and parsing the log file\")\n",
    "\n",
    "# Group by process_id and collect log entries\n",
    "processes = defaultdict(list)\n",
    "for entry in log_entries:\n",
    "    processes[entry[\"process_id\"]].append((entry[\"from_server\"], entry[\"to_server\"], entry[\"time\"], entry[\"action\"]))\n",
    "\n",
    "logging.info(\"Grouped by process_id and collected log entries\")\n",
    "\n",
    "# Convert events to server sequences ignoring timestamps\n",
    "def concat_events(events):\n",
    "    return [\"{}_{}\".format(event[0], event[1]) for event in events]\n",
    "\n",
    "process_events_dict = {pid: (concat_events(events), events) for pid, events in processes.items()}\n",
    "\n",
    "# Function to calculate edit distance using the editdistance package\n",
    "def calculate_edit_distance(seq1, seq2):\n",
    "    return editdistance.eval(seq1, seq2)\n",
    "\n",
    "# Generate candidate pairs for part 2 (edit distance < 4)\n",
    "candidate_pairs_part2 = []\n",
    "for pid1 in tqdm(process_events_dict):\n",
    "    for pid2 in process_events_dict:\n",
    "        if pid1 < pid2:\n",
    "            seq1 = process_events_dict[pid1][0]\n",
    "            seq2 = process_events_dict[pid2][0]\n",
    "            if calculate_edit_distance(seq1, seq2) < 4:\n",
    "                candidate_pairs_part2.append((pid1, pid2))\n",
    "\n",
    "logging.info(\"Filtered candidates for part 2 based on edit distance < 4\")\n",
    "\n",
    "# Further filter the part 2 candidates for part 1 (edit distance < 2)\n",
    "candidate_pairs_part1 = [pair for pair in candidate_pairs_part2 if calculate_edit_distance(process_events_dict[pair[0]][0], process_events_dict[pair[1]][0]) < 2]\n",
    "\n",
    "logging.info(\"Filtered candidates for part 1 based on edit distance < 2\")\n",
    "\n",
    "# Group similar pairs to form clusters\n",
    "def merge_clusters(clusters):\n",
    "    merged_groups = []\n",
    "    seen = set()\n",
    "    for key, group in clusters.items():\n",
    "        if key not in seen:\n",
    "            merged_group = {key} | set(group)\n",
    "            to_merge = [g for g in merged_groups if g & merged_group]\n",
    "            for g in to_merge:\n",
    "                merged_group |= g\n",
    "                merged_groups.remove(g)\n",
    "            merged_groups.append(merged_group)\n",
    "            seen.update(merged_group)\n",
    "    return merged_groups\n",
    "\n",
    "def form_clusters(candidate_pairs):\n",
    "    clusters_dict = defaultdict(set)\n",
    "    for k, v in candidate_pairs:\n",
    "        clusters_dict[k].add(v)\n",
    "        clusters_dict[v].add(k)\n",
    "    return merge_clusters(clusters_dict)\n",
    "\n",
    "merged_groups_part1 = form_clusters(candidate_pairs_part1)\n",
    "merged_groups_part2 = form_clusters(candidate_pairs_part2)\n",
    "\n",
    "# Log number of clusters\n",
    "num_clusters_part1 = len(merged_groups_part1)\n",
    "num_clusters_part2 = len(merged_groups_part2)\n",
    "logging.info(f\"Number of clusters for part 1: {num_clusters_part1}\")\n",
    "logging.info(f\"Number of clusters for part 2: {num_clusters_part2}\")\n",
    "\n",
    "# Calculate and log average cluster size\n",
    "average_cluster_size_part1 = sum(len(group) for group in merged_groups_part1) / num_clusters_part1\n",
    "average_cluster_size_part2 = sum(len(group) for group in merged_groups_part2) / num_clusters_part2\n",
    "logging.info(f\"Average cluster size for part 1: {average_cluster_size_part1}\")\n",
    "logging.info(f\"Average cluster size for part 2: {average_cluster_size_part2}\")\n",
    "\n",
    "logging.info(\"Merged similar groups into clusters\")\n",
    "\n",
    "# Function to get the representative process\n",
    "def get_representative_process(group):\n",
    "    min_total_distance = float('inf')\n",
    "    representative_process = None\n",
    "    for pid1 in group:\n",
    "        total_distance = 0\n",
    "        for pid2 in group:\n",
    "            if pid1 != pid2:\n",
    "                total_distance += calculate_edit_distance(process_events_dict[pid1][0], process_events_dict[pid2][0])\n",
    "        if total_distance < min_total_distance:\n",
    "            min_total_distance = total_distance\n",
    "            representative_process = pid1\n",
    "    return representative_process\n",
    "\n",
    "# Generate part1Output.txt\n",
    "def generate_part1_output(merged_groups, output_file_path):\n",
    "    with open(output_file_path, 'w') as file:\n",
    "        new_id = 1\n",
    "        for group in merged_groups:\n",
    "            representative_process = get_representative_process(group)\n",
    "            merged_pids = \",\".join(map(str, sorted(group)))\n",
    "            file.write(f\"{new_id}:{merged_pids}\\n\")\n",
    "            _, events = process_events_dict[representative_process]\n",
    "            for event in events:\n",
    "                file.write(f\"<{event[0]}, {event[1]}, {event[2]}, {event[3]}, {representative_process}>\\n\")\n",
    "            new_id += 1\n",
    "    logging.info(f\"Generated part1Output.txt: {output_file_path}\")\n",
    "\n",
    "generate_part1_output(merged_groups_part1, part1_output_path)\n",
    "\n",
    "# Generate part1Observations.txt\n",
    "def generate_part1_observations(merged_groups, output_file_path):\n",
    "    with open(output_file_path, 'w') as file:\n",
    "        for group in merged_groups:\n",
    "            file.write(f\"Group: {sorted(group)}\\n\")\n",
    "            for pid in sorted(group):\n",
    "                file.write(f\"{pid}:\\n\")\n",
    "                _, events = process_events_dict[pid]\n",
    "                for event in events:\n",
    "                    file.write(f\"<{event[0]}, {event[1]}, {event[2]}, {event[3]}, {pid}>\\n\")\n",
    "            file.write(\"\\n\")\n",
    "    logging.info(f\"Generated part1Observations.txt: {output_file_path}\")\n",
    "\n",
    "generate_part1_observations(merged_groups_part1, part1_observations_path)\n",
    "\n",
    "# Generate part2Observations.txt\n",
    "def generate_part2_observations(merged_groups, output_file_path):\n",
    "    with open(output_file_path, 'w') as file:\n",
    "        for group in merged_groups:\n",
    "            file.write(f\"Group: {sorted(group)}\\n\")\n",
    "            for pid in sorted(group):\n",
    "                file.write(f\"{pid}:\\n\")\n",
    "                _, events = process_events_dict[pid]\n",
    "                for event in events:\n",
    "                    file.write(f\"<{event[0]}, {event[1]}, {event[2]}, {event[3]}, {pid}>\\n\")\n",
    "            file.write(\"\\n\")\n",
    "    logging.info(f\"Generated part2Observations.txt: {output_file_path}\")\n",
    "\n",
    "generate_part2_observations(merged_groups_part2, part2_observations_path)\n",
    "\n",
    "logging.info(\"Process completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3919dbae-cf5d-4c75-98aa-98087cf2d4a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
